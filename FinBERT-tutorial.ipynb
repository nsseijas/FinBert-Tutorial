{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c129f9f-d87b-43f6-b96b-703bfaf42eda",
   "metadata": {},
   "source": [
    "# FinBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d47cc7-ea19-4ff5-81c6-7a45c14cc69f",
   "metadata": {},
   "source": [
    "In this notebook, you will learn how to use **FinBERT**, a transformer-based model fine-tuned on financial text, to perform sentiment analysis. We will cover how to: \n",
    "- Set up FinBERT using the Hugging Face Transformers library\n",
    "- Run predictions on financial text\n",
    "- Interpret and visualize sentiment results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8157f0b6-a9ed-4391-99ae-ca8fbbeb9a31",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Natural language processing (NLP) has entered a new era in how machines represent and understand language. But let's first recap the evolution of language representation for classification tasks, like **sentiment analysis**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed2706b-f184-4382-80b4-2993d028bf39",
   "metadata": {},
   "source": [
    "## Early Sentiment Classification: N-Grams, Bag-of-Words, and Linear Models\n",
    "The earliest approaches to sentiment analysis relied on simple yet effective techniques rooted in **n-gram** models and the **Bag-of-Words (BoW)** representation.  N-grams represent documents by counting sequences of one (unigram), two (bigram), or more consecutive words. For example, “not good” as a bigram helps capture negative sentiment more accurately than the words “not” and “good” alone.\n",
    "In the BoW model, each document is converted into a sparse vector representing the frequency of words or n-grams, without capturing grammar or word meaning. These patterns are counted and used as features to train classification models. These vectors are then used in simple classifiers like:\n",
    "- **Naive Bayes** - assumes word independence and applies Bayes’ theorem to predict sentiment by modeling the likelihood of observing each word in documents of a particular sentiment.\n",
    "- **Logistic Regression** - learns weights for each word or n-gram as input features to predict sentiment probability.\n",
    "\n",
    "These models are fast and  effective for simple cases, but while n-gram models can capture how often words appear together in sequence, they treat each word in isolation, lack the ability to generalize across contexts or capture deeper word meanings. Words like “bad” and “terrible” were treated as unrelated, and sentence structure was lost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68cc887-8d1f-4283-be89-7edc312160be",
   "metadata": {},
   "source": [
    "## Machine Learning \n",
    "\n",
    "In the mid-2000s, NLP saw a shift with the emergence of machine learning techniques like **Support Vector Machines (SVMs)** and **Conditional Random Fields (CRFs)**. These models offered stronger generalization and allowed incorporation of more complex linguistic features into classification tasks. However, they still relied on sparse, hand-crafted features—typically derived from BoW or n-gram representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0120ff5b-79ef-449c-a923-dfa80b11f5e7",
   "metadata": {},
   "source": [
    "## Neural Networks - Non-Linear Classification\n",
    "To overcome the limitations of linear models, **neural networks** were introduced. A basic neural network consists of an input layer, one or more hidden layers that apply nonlinear transformations, and an output layer that that produces predictions.\n",
    "\n",
    "Neural networks allow models to learn more complex decision boundaries and interactions between features. \n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/0*KfJUyVjsS9ZhxcBk.png\" alt=\"Classical vs Deep Learning\" width=\"600\">\n",
    "Image Source: Adarsha Regmi, NLP guide towards neural networks\n",
    "\n",
    "  \n",
    "Neural language models represent words in this prior context by their embeddings, rather than just by their word identity as used in n-gram language models. Compared to n-gram models, neural language models can handle much longer histories, can generalize better over contexts of similar words, and are more accurate at word-prediction. On the other hand, neural net language models are much more complex, are slower and need more energy to train, and are less interpretable than n-gram models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd115bb2-50cb-4011-9063-d2823b7e6445",
   "metadata": {},
   "source": [
    "## Word Embeddings and Deep Learning\n",
    "### Static word Embeddings\n",
    "In vector semantics, a word is modeled as a vector in multi-dimensional continuous space, called an **embedding**. Traditional word embedding models, such as **Word2Vec** and **GloVe**, moved NLP forward by mapping words to dense vector representations that capture semantic and syntactic relationships. The embeddings are learned from the word distributions in large text corpora, and allow models to place similar words close in vector space (e.g., “excellent” and “great”), which significantly improves sentiment classification. \n",
    "\n",
    "<img src=\"https://towardsdatascience.com/wp-content/uploads/2021/03/15F4TXdFYwqi-BWTToQPIfg.jpeg\" alt=\"Word2Vec Vectors\" width=\"600\">\n",
    "Image source: Word2Vec Research Paper Explained. Toward Data Science\n",
    "\n",
    "Google’s **Word2Vec** introduced two training architectures:\n",
    "- **CBOW** (Continuous Bag-of-Words)- predicts a word given its context.\n",
    "- **Skip-Gram** - predicts context words given a target word.\n",
    "\n",
    "**GloVe**, developed at Stanford, took a matrix factorization approach, learning embeddings from global word co-occurrence counts. Like Word2Vec, it produced static embeddings that improved many NLP tasks, including sentiment analysis.\n",
    "\n",
    "Both rely on shallow neural networks to learn embeddings—dense vector representations that place semantically similar words close together. For sentiment classification, this meant models could now recognize that “happy” and “joyful” convey similar tone.\n",
    "\n",
    "\n",
    "\n",
    "Since these models were pre-trained, the embeddings can be reused across tasks, reducing the need for task-specific training. However, a key limitation of these models is that each word has a single fixed vector regardless of context, i.e. **static embeddings**. A word like “cold” has the same vector whether referring to weather or tone.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "663a68c2-61d8-4451-85b8-4896a150da43",
   "metadata": {},
   "source": [
    "### Contextual Word Embeddings\n",
    "\n",
    "These limitations prompted the development of more advanced **contextualized word embeddings**. Models like Embeddings from Language Models (**ELMo**) addressed this by using bi-directional (**biLM**) Long Short-Term Memory (**LSTMs**) to generate word vectors that change based on surrounding context. The model analyzes full sentence context and generates dynamic word representations. The same word—e.g., “charged”—could now have different representations in “charged a fee” vs. “charged with a crime.”\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/elmo-forward-backward-language-model-embedding.png\" alt=\"ELMo Embeddings\" width=\"500\">\n",
    "Image source: Jay Alammar, The Illustrated BERT, ELMO, and co. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb63285-3797-46af-8de2-79fcfc9179e0",
   "metadata": {},
   "source": [
    "### Transfer Learnings\n",
    "\n",
    "**ULMFiT** followed with innovations in transfer learning, showing how a pretrained language model could be fine-tuned for specific downstream NLP tasks such as sentiment classification or spam detection. These models trained on vasts amounts of unlabeled text to predict the next word in a sequence. Thus making it possible to learn deep language patterns without the need for manual annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5e3592-2682-4cc7-afba-d4803e12a149",
   "metadata": {},
   "source": [
    "## Transformers and Large Language Models\n",
    "\n",
    "The Transformer architecture replaced recurrence with self-attention, enabling efficient parallel processing and capturing long-range dependencies in text.. This design became the basis for **large language models (LLMs)** like GPT and BERT.\n",
    "\n",
    "These models are pretrained on massive corpora using unsupervised tasks such as masked language modeling (BERT) or next-word prediction (GPT), and fine-tuned for downstream tasks like sentiment classification. Unlike earlier methods, LLMs produce contextual embeddings that adapt to the full sentence, enabling precise classification of sentiment, tone, and subtle meaning.\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/t/Transformer_decoder.png\" alt=\"Transformer\" width=\"500\">\n",
    "Image source: Jay Alammar, The Illustrated Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e72112-3979-4335-aac9-6fa120fff557",
   "metadata": {},
   "source": [
    "### BERT: Deep Bidirectional Context for Sentiment Tasks\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) marked another breakthrough. By analyzing text in both directions, BERT captures richer context than unidirectional models. Pretrained on Wikipedia and BookCorpus, it can be fine-tuned on sentiment classification.\n",
    "\n",
    "BERT significantly outperforms earlier models on sentiment benchmarks. Its embeddings are dynamic and encode nuanced information, allowing it to distinguish “not bad” from “bad” or understand sarcasm and negation. It uses the Transformer encoder to process each word in relation to both preceding and following words.\n",
    "\n",
    "BERT introduced two key innovations:\n",
    "1. Masked Language Modeling (MLM): randomly masks words and predicts them, enabling deep context learning.\n",
    "2. Transformer architecture: captures long-term dependencies better than RNNs.\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/BERT-classification-spam.png\" alt=\"Transformer\" width=\"500\">\n",
    "Image source: Jay Alammar, The Illustrated BERT, ELMO, and co. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0fa3be-8bcd-4784-8f89-eac1c1179229",
   "metadata": {},
   "source": [
    "#### FinBERT: Domain-Specific Sentiment Classification in Finance\n",
    "\n",
    "While BERT performs well on general text, it struggles with financial language where words like “negative,” “flat,” or “beat” have technical meanings. For example, “negative cash flow” is a neutral financial term, not an expression of negative sentiment, and “beat consensus” implies a positive outcome, not aggression.\n",
    "\n",
    "FinBERT adapts BERT by pretraining it on 4.9 billion tokens from SEC filings, earnings calls, and analyst reports. This domain-specific training helps it better understand financial jargon, tone, and context—allowing for more accurate sentiment classification in tasks like investor communication analysis and earnings report interpretation. FinBERT consistently outperforms general models on financial NLP tasks.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*-qwGn_js-CjwgOz5e8tWug.png\" alt=\"FinBERT1\" width=\"400\">\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*WBvoOrHXYXHPy_tx8iwLOQ.png\" alt=\"FinBERT2\" width=\"250\">\n",
    "\n",
    "Image source: Zulkuf Genc, FinBERT: Financial Sentiment Analysis with BERT \n",
    "\n",
    "All the fine-tuned FinBERT models are publicly hosted at Huggingface 🤗."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b163b4-a1ae-4fbc-8488-741641c312e4",
   "metadata": {},
   "source": [
    "\n",
    "References: \n",
    "- [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) (Dan Jurafsky and James H. Martin. 2025)\n",
    "- Word2Vec [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781) (Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean. 2013)\n",
    "- [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/) (Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014)\n",
    "- ELMo [Deep contextualized word representations](https://arxiv.org/pdf/1802.05365) (Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018)\n",
    "- BERT [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805) (Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. 2019)\n",
    "- FinBERT [FinBERT: Financial Sentiment Analysis with BERT](https://medium.com/prosus-ai-tech-blog/finbert-financial-sentiment-analysis-with-bert-b277a3607101) HugginFace, ProsusAI/finbert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24428709-ab01-4eb0-a215-e1a0d0665d4c",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8970c1-d1e5-489d-8083-5dbd8acfd403",
   "metadata": {},
   "source": [
    "## Required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b55565a-0f6a-4507-8d93-a5d6ed5c684d",
   "metadata": {},
   "source": [
    "To follow along with this tutorial, you will need the following Python libraries:\n",
    "- `transformers`: from Hugging Face, to load FinBERT\n",
    "- `torch`: for running the model (PyTorch based)\n",
    "- `pandas`\n",
    "- `numpy`\n",
    "- `matplotlib`\n",
    "- `scipy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38e93c8-fa10-4b6a-b377-638d3e665786",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Please follow the instructions in the `README.md` file to set up the environment using `conda` and `environment.yml`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0812720-486e-4005-baab-a68bc09f21a2",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d35727e-b7d7-4e0c-8485-627282258a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set display options for pandas\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a636dcb-0b62-416a-a0b9-032f42b8cc02",
   "metadata": {},
   "source": [
    "This code checks your system's PyTorch library and the Transformers Hugging Face installation and whether CUDA (NVIDIA GPU support) is available for acceleration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22080c31-32da-43fc-89c2-87677a9505dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.0.1\n",
      "CUDA available: False\n",
      "Transformers version: 4.40.1\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Transformers version:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ed9255-bd00-437c-ad56-8dde117e13a4",
   "metadata": {},
   "source": [
    "## Load FinBERT and Tokenizer\n",
    "\n",
    "As, mentioned above, FinBERT is a version of BERT that has been fine-tuned specifically on financial sentiment data such as earnings reports, analyst statements, and press releases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "228cd2f9-a710-41ad-b6f8-e6472ef8540d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e6193ee-a298-4a8a-8b4e-3533c1fd5f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained FinBERT model and tokenizer\n",
    "model_name = \"ProsusAI/finbert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eb5d87-d0b6-4966-bf80-283b2a7be1a4",
   "metadata": {},
   "source": [
    "The `tokenizer` is what will split the text into tokens and converts them into numeric input IDs. The `model` will process those IDs and outputs raw scores for three sentiment classes: positive, negative, and neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4d2446-d5e9-455c-b743-d72423eabf7a",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "Analyzing sentiment in financial text is valuable for understanding the perspectives of managers, analysts, and investors. \n",
    "\n",
    "- **Input**: A financial text.\n",
    "\n",
    "- **Output**: Positive, Neutral or Negative.\n",
    "\n",
    "Let’s analyze some sample financial statements. In practice, these could be headlines, 10-K filings, press releases, etc.\n",
    "\n",
    "### Sentence examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fdddb70-9976-4cb8-bd85-1ae01fd76d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    # Positive examples\n",
    "    \"Earnings exceeded analyst expectations by 12%.\",\n",
    "    \"The new product line drove record profits this quarter.\",\n",
    "\n",
    "    # Negative examples\n",
    "    \"The company faces lawsuits related to data breaches.\",\n",
    "    \"Supply chain disruptions severely impacted Q2 margins.\",\n",
    "\n",
    "    # Neutral examples\n",
    "    \"The board of directors met on May 3rd.\",\n",
    "    \"The company is headquartered in San Jose, California.\",\n",
    "\n",
    "    # Ambiguous example\n",
    "    \"Despite strong revenue, net income declined due to increased R&D investment.\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a410a844-4724-4514-bdef-8870b7d7dc67",
   "metadata": {},
   "source": [
    "### Tokenize the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb841560-fc0a-405d-81db-bb26bb10c261",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3114fafd-0a30-4488-99f3-1149ebbcfb0c",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- `padding=True`: ensures all sentences are the same length.\n",
    "- `truncation=True`: cuts off long sentences that exceed the model’s max input length.\n",
    "- `return_tensors=\"pt\"`: returns PyTorch tensors for use with the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eb6283-c429-4bde-9122-50c77d03ce3a",
   "metadata": {},
   "source": [
    "## Run Sentiment Predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7c6d15-17c5-4bf0-b653-bdbb50273705",
   "metadata": {},
   "source": [
    "### Get Sentiment Predictions\n",
    "\n",
    "Now, we’ll pass the tokenized text to the FinBERT model and interpret the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7611dc6-14ed-4177-9df4-eaf997a3af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbacc31-c3f9-4622-9506-b8bddeafdaa7",
   "metadata": {},
   "source": [
    "What is happening:\n",
    "- We use `torch.no_grad()` because we’re only making predictions, not training.\n",
    "- The model returns **logits**, which are unnormalized prediction scores.\n",
    "- We convert logits to **probabilities** using the softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0405f5f9-a6c8-4e7a-8526-9a234d56beed",
   "metadata": {},
   "source": [
    "### Map Predictions to Sentiment Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e54892b-c661-44db-8cca-b4c493a23f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Earnings exceeded analyst expectations by 12%.\" → positive (0.95)\n",
      "\"The new product line drove record profits this quarter.\" → positive (0.93)\n",
      "\"The company faces lawsuits related to data breaches.\" → negative (0.91)\n",
      "\"Supply chain disruptions severely impacted Q2 margins.\" → negative (0.97)\n",
      "\"The board of directors met on May 3rd.\" → neutral (0.94)\n",
      "\"The company is headquartered in San Jose, California.\" → neutral (0.95)\n",
      "\"Despite strong revenue, net income declined due to increased R&D investment.\" → negative (0.97)\n"
     ]
    }
   ],
   "source": [
    "labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "for text, prob in zip(texts, probs):\n",
    "    sentiment = labels[torch.argmax(prob)]\n",
    "    confidence = prob.max().item()\n",
    "    print(f\"\\\"{text}\\\" → {sentiment} ({confidence:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf142ad-6182-4262-af41-7cffb92265c5",
   "metadata": {},
   "source": [
    "How do we get the scores?\n",
    "- `torch.argmax(probs)` gives the index of the highest probability class.\n",
    "- We map that index to one of the labels: \"positive\", \"negative\", or \"neutral\".\n",
    "- Confidence is how strongly the model believes in its prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afa733f-de61-4480-b00b-efe6019f1d09",
   "metadata": {},
   "source": [
    "## Try Domain-Specific Text Snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc523bc0-e4f7-4274-95f8-df4c48cf1212",
   "metadata": {},
   "source": [
    "### Analyst notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed14aa10-73a9-4383-ae95-7777ad83f089",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"We maintain our overweight rating due to improving margins.\",\n",
    "    \"Downgraded to neutral as macro risks continue to weigh on sentiment.\",\n",
    "    \"The upgrade reflects stronger-than-expected earnings and a favorable outlook.\",\n",
    "    \"We remain cautious due to continued pricing pressure and regulatory overhang.\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b478dad-5901-441f-a58a-1e4761cb752a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"We maintain our overweight rating due to improving margins.\" → positive (0.96)\n",
      "\"Downgraded to neutral as macro risks continue to weigh on sentiment.\" → negative (0.89)\n",
      "\"The upgrade reflects stronger-than-expected earnings and a favorable outlook.\" → positive (0.96)\n",
      "\"We remain cautious due to continued pricing pressure and regulatory overhang.\" → negative (0.90)\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "for text, prob in zip(texts, probs):\n",
    "    sentiment = labels[torch.argmax(prob)]\n",
    "    confidence = prob.max().item()\n",
    "    print(f\"\\\"{text}\\\" → {sentiment} ({confidence:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca595b0a-a5fe-41e7-8f94-af6bf63494d4",
   "metadata": {},
   "source": [
    "### Earnings call quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54526cd4-6604-4786-830a-e376de19d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"We experienced strong year-over-year revenue growth across all segments.\",\n",
    "    \"Operating margins declined this quarter due to higher logistics costs.\",\n",
    "    \"We are reaffirming our full-year guidance despite market volatility.\",\n",
    "    \"Customer demand remained stable, but input costs continue to rise.\",\n",
    "    \"We anticipate headwinds from FX and interest rate uncertainty in Q3.\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ce694bf-4248-4d98-8b21-e1e13872ef6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"We experienced strong year-over-year revenue growth across all segments.\" → positive (0.96)\n",
      "\"Operating margins declined this quarter due to higher logistics costs.\" → negative (0.98)\n",
      "\"We are reaffirming our full-year guidance despite market volatility.\" → positive (0.94)\n",
      "\"Customer demand remained stable, but input costs continue to rise.\" → positive (0.89)\n",
      "\"We anticipate headwinds from FX and interest rate uncertainty in Q3.\" → negative (0.95)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "for text, prob in zip(texts, probs):\n",
    "    sentiment = labels[torch.argmax(prob)]\n",
    "    confidence = prob.max().item()\n",
    "    print(f\"\\\"{text}\\\" → {sentiment} ({confidence:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42530206-9077-4235-b2db-0b018755d721",
   "metadata": {},
   "source": [
    "### Risk factor excerpts from 10-Ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f85126d-5553-4142-bf8d-47d923a0a729",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Our business could be adversely affected by rising interest rates.\",\n",
    "    \"We face ongoing risks related to global supply chain disruptions.\",\n",
    "    \"Failure to comply with new ESG regulations may impact operations.\",\n",
    "    \"A prolonged downturn in consumer spending could hurt sales performance.\",\n",
    "    \"We are exposed to cybersecurity threats that may disrupt critical systems.\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff694c28-78c4-4579-90f2-9cd471e5acb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Our business could be adversely affected by rising interest rates.\" → negative (0.91)\n",
      "\"We face ongoing risks related to global supply chain disruptions.\" → negative (0.95)\n",
      "\"Failure to comply with new ESG regulations may impact operations.\" → negative (0.95)\n",
      "\"A prolonged downturn in consumer spending could hurt sales performance.\" → negative (0.97)\n",
      "\"We are exposed to cybersecurity threats that may disrupt critical systems.\" → negative (0.84)\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "for text, prob in zip(texts, probs):\n",
    "    sentiment = labels[torch.argmax(prob)]\n",
    "    confidence = prob.max().item()\n",
    "    print(f\"\\\"{text}\\\" → {sentiment} ({confidence:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e976e0-967a-478d-97b3-e3b2a1251fed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (finbert)",
   "language": "python",
   "name": "finbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
